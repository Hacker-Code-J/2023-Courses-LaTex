\documentclass[12pt,openany]{book}
\usepackage{kotex}
\usepackage{amsmath,amsthm,amsfonts,amscd} % Packages for mathematics
\usepackage{commath} %absolute value
% Colors
\usepackage[dvipsnames]{xcolor}
\definecolor{titleblue}{RGB}{0,53,128}
\definecolor{chaptergray}{RGB}{140,140,140}
\definecolor{sectiongray}{RGB}{180,180,180}

\definecolor{thmcolor}{RGB}{231, 76, 60}
\definecolor{defcolor}{RGB}{52, 152, 219}
\definecolor{lemcolor}{RGB}{155, 89, 182}
\definecolor{corcolor}{RGB}{46, 204, 113}
\definecolor{procolor}{RGB}{241, 196, 15}

% Fonts
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newpxtext,newpxmath}
\usepackage{sectsty}
\allsectionsfont{\sffamily\color{titleblue}\mdseries}

% Page layout
\usepackage{geometry}
\geometry{a4paper,left=1in,right=.7in,top=1in,bottom=1in,heightrounded}
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO]{\nouppercase{\rightmark}}
\fancyhead[RE]{\nouppercase{\leftmark}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Chapter formatting
\usepackage{titlesec}
\titleformat{\chapter}[display]
{\normalfont\sffamily\Huge\bfseries\color{titleblue}}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat{\section}
{\normalfont\sffamily\Large\bfseries\color{titleblue!100!gray}}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\sffamily\large\bfseries\color{titleblue!75!gray}}{\thesubsection}{1em}{}

% Table of contents formatting
\usepackage{tocloft}
\renewcommand{\cftchapfont}{\sffamily\color{titleblue}\bfseries}
\renewcommand{\cftsecfont}{\sffamily\color{chaptergray}}
\renewcommand{\cftsubsecfont}{\sffamily\color{sectiongray}}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}

\usepackage{cancel}
\newcommand\crossout[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\ncrossout[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
% Hyperlinks
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=titleblue,
	filecolor=black,      
	urlcolor=titleblue,
}

%Ceiling and Floor Function
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

%Algorithm
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{algpseudocode}
\SetKwComment{Comment}{/* }{ */}
\SetKw{Break}{break}
\SetKw{Downto}{downto}
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{KeyGen}{KeyGen}



%---------------------------My Preamble
\usepackage{marvosym} %Lightning
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{tabularx}
\setlength{\columnsep}{2cm}
\setlength{\columnseprule}{1.25pt}
\usepackage{enumerate}
\usepackage{soul}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{calc}
\usetikzlibrary{arrows, arrows.meta, positioning, shapes.multipart}
\usepackage{pgfplots}
\usetikzlibrary{patterns}
\pgfplotsset{compat=newest}

%Tcolorbox
\usepackage[most]{tcolorbox}
\tcbset{colback=white, arc=5pt}
%\tcbset{enhanced, colback=white,colframe=black,fonttitle=\bfseries,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}}
%White box with black text and shadow
%\begin{tcolorbox}[colback=white,colframe=black,fonttitle=\bfseries,title=Black Shadow Box,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}]
%	This is a white box with black text and a subtle shadow. The shadow adds some depth and dimension to the box without overpowering the design.
%\end{tcolorbox}

%Theorem
%\newtheorem{axiom}{Axiom}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem*{note}{Note}
\newtheorem*{axiom}{Axiom}

%New Command
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\dispsty}{\displaystyle}
\newcommand{\sol}{\textcolor{magenta}{\bf Solution}}
\newcommand{\eg}{\textnormal{e.g.}}
\newcommand{\ie}{\textnormal{i.e.}}

\newcommand{\Var}{\text{Var}}
\newcommand{\sd}{\text{sd}}
\newcommand{\mean}[1]{\bar{#1}}
\newcommand{\Cov}{\textit{Cov}}
\newcommand{\Corr}{\textit{Corr}}
\newcommand{\SE}{\text{S.E.}}
\newcommand{\df}{\text{d.f.}}

\newcommand{\markov}[1]{\langle #1\rangle}
\newcommand{\tab}{\hspace{12pt}}

% Begin document
\begin{document}
	
	% Title page
	\begin{titlepage}
		\begin{center}
			{\Huge\textsf{\textbf{Theory of Random Number Generation}}\par}
			\vspace{0.5in}
			{\Large Ji Yong-Hyeon\par}
			\vspace{1in}
			\includegraphics[scale=2]{rng.jpg}\par
			\vspace{1in}\large
			{\bf Department of Information Security, Cryptology, and Mathematics\par}
			{College of Science and Technology\par}
			{Kookmin University\par}
			%\includegraphics[width=1.5in]{school_logo.jpg}\par
			\vspace{.25in}
			{\large \today\par}
		\end{center}
	\end{titlepage}
	
	% Table of contents
	\tableofcontents
	
	% Chapters
	\mainmatter
	
	\chapter{Introduction}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Summary}]
		\begin{itemize}
			\item Required Properties for Random Bit Generator
			\begin{itemize}
				\item \textbf{Unpredictability}, \textbf{Unbiasedness}, \textbf{Independence}
			\end{itemize}
			\item Components of Cryptographically Secure Random Bit Generator
			\begin{itemize}
				\item TRNG (Entropy Source) + PRNG (Pseudorandom Number Generator)
			\end{itemize}
			\item Methods for Evaluating the Security of Random Bit Generator
			\begin{itemize}
				\item Estimation of entropy for the output sequence from TRNG
				\item Statistical randomness tests for the output sequence from RNG
			\end{itemize}
			\item Types of Random Bit Generators
			\begin{itemize}
				\item Hardware/Software-based Random Bit Generators
				\item Operating System-based Random Bit Generators
				\item Various Standard Pseudorandom Number Generators
			\end{itemize}
		\end{itemize}
	\end{tcolorbox}
	Functions of RBG (Random Bit Generator)
	
	Provides random numbers required for cryptographic systems
	An essential element (algorithm) for the operation of cryptographic systems and modules
	Required Properties: Unpredictability, Unbiasedness, Independence between bits
	
	Ideally, the output should be akin to the results of "coin tossing."
	Applications of Random Bit Generator
	
	Generation of Key and Initialization Vector (IV) used in symmetric-key cryptography (block/stream ciphers)
	Generation of various parameters in public-key cryptography: prime number generation, probabilistic public-key cryptography, etc.
	Generation of various parameters used in cryptographic protocols: nonce, salt, etc.
	
	\newpage
	\chapter{Probability Theory}
	
	\section{Introduction}
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf }]
		\begin{definition}
			\ \begin{itemize}
				\item An \textbf{experiment} is the process of observing a phenomenon that has variation in its outcomes.
				\item The \textbf{sample space} $S$ associated with an experiment is \underline{the collection} of all
				possible distinct outcomes of the experiment.
				\item An \textbf{event} $A, B$ is the set of elementary outcomes possessing a designated feature. ($A,B\subseteq S$)
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{itemize}
			\item Union: $A\cup B$
			\item Complement: $A^C$
			\item Intersection: $A\cap B$ (simply, $AB$)
			\item $A,B$ are mutually disjoint $\iff A\cap B=\emptyset$
		\end{itemize}
	\end{remark}
	
	\newpage
	\section{Axioms of Probability}
	\subsection{Kolmogorov's Axiom}
	\begin{tcolorbox}[colback=white,colframe=magenta,arc=5pt,title={\color{white}\bf Kolmogorov's Axiom}]
		\begin{axiom}
			The probability is a function $\Pr:2^\Omega\to[0,1]\subseteq\R$ satisfies
			\begin{enumerate}[(\text{A}1)]
				\item $\forall$\text{event} $A$, $0\leq\Pr[A]\leq 1$.
				\item $\Pr[\Omega]=1$.
				\item (Countable Additivity) $
				P\del{\bigcup_{i=1}^\infty A_i}=\sum_{i=1}^\infty P[A_i],
				$ where $\set{A_1,A_2,\dots}$ is a countable set.
			\end{enumerate}
		\end{axiom}
	\end{tcolorbox}
	\begin{remark}
		A probability is a function $\Pr:2^{\Omega}\to[0,1]\subseteq\R$.
	\end{remark}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf}]
		\begin{proposition}
			Let $A,B\subseteq\Omega$. \begin{enumerate}[(1)]
				\item $\Pr[A]=\Pr[AB^C]+\Pr[AB]$
				\item $\Pr[B]=\Pr[AB]+\Pr[A^CB]$
				\item $\Pr[A\cup B]=\Pr[A]+\Pr[B]-\Pr[AB]$
				\item $\Pr[A\cup B]=\Pr[AB^C]+\Pr[AB]+\Pr[A^CB]$
				\item $\Pr[A^C]=1-\Pr[A]$
				\item $A\subseteq B\implies\Pr[A]\leq\Pr[B]$
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	
	\newpage
	\subsection{Conditional Probability and Independent}
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Conditional Probability}]
		\begin{definition}
			The \textbf{conditional probability} of $A$ given $B$ is denoted by $\Pr[A|B]$ and defined by the formula
			\[
			\Pr[A|B] = \frac{\Pr[AB]}{\Pr[B]}\quad\text{with}\quad\Pr[B]>0.
			\] Equivalently, this formula can be written as \textbf{multiplication law of probability}:\[
			\Pr[AB] = \Pr[A|B]\Pr[B].
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{example}
	\ \begin{enumerate}[(1)]
		\item Start with a \textit{shuffled deck of cards} and distribute all 52 cards to 4 player, 13 cards to each. What is the probability that each player gets an Ace?
		\item Next, assume that you are a player and you get a single Ace. What is the probability now that each player gets an Ace?
	\end{enumerate}
	\begin{proof}[\sol]
		\ \begin{enumerate}[(1)]
			\item If any ordering of cards is equally likely, then any position of the four Aces in the deck is also equally likely. There are \[
			\binom{52}{4}=\frac{52!}{4!48!}
			\] possibilities for the positions (slots) for the 4 aces. Out of these, the number of positions that give each player an Ace $13^4$ pick the first slot among the cards that the first player gets, then the second slot among the second player's card, then the third and the fourth slot. Therefore, the answer is $
			\frac{13^4}{\binom{52}{4}}\approx0.1055.
			$
			\item After you see that you have a single Ace, the probability goes up the previous answer need to be divided by the probability that you get a single Ace, which is \[
			\frac{\dispsty13\cdot\binom{39}{3}}{\dispsty\binom{52}{4}}\approx0.4388.
			\] Note that \[
			P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A)}{P(B)}.
			\] The answer then becomes $
			\frac{13^4}{13\cdot\binom{39}{3}}\approx0.2404.
			$
		\end{enumerate}
	\end{proof}
\end{example}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Independence}]
		\begin{definition}
			Two events $A$ and $B$ are \textbf{independent} if \[
			\Pr[A|B] = \Pr[A]
			\] Equivalent conditions are \[
			\Pr[B|A] = \Pr[B]\quad\text{or}\quad \Pr[AB]=\Pr[A]\Pr[B]
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		$\displaystyle\Pr[A]=\Pr[A|B]=\frac{\Pr[AB]}{\Pr[B]}\implies \Pr[AB]=\Pr[A]\Pr[B]$.
	\end{remark}
	\vspace{10pt}
	\begin{example}
		Suppose we roll a dice once. Let the universal set is $U=\set{1,2,3,4,5,6}$.
		\begin{enumerate}[(1)]
			\item (Independent but Not Disjoint) Let \[
			A=\set{1,3,5}\quad\text{and}\quad B=\set{3,6}.
			\] Then $A\cap B=\set{3}\neq\emptyset$, that is, $A$ and $B$ are not disjoint. Note that \begin{align*}
				\Pr[A]=\frac{3}{6}=\frac{1}{2},\quad&\Pr[B]=\frac{2}{6}=\frac{1}{3},\\
				\Pr[A\mid B]=\frac{\Pr[AB]}{\Pr[B]}=\frac{1/6}{1/3}=\frac{1}{2},\quad&\Pr[B\mid A]=\frac{\Pr[BA]}{\Pr[A]}=\frac{1/6}{1/2}=\frac{1}{3}.
			\end{align*} Thus, $\Pr[A|B]=\Pr[A]$ and $\Pr[B|A]=\Pr[B]$. That is, $A$ and $B$ are mutually independent.
			\item (Not Independent but Disjoint) Let \[
			A=\set{1,3,5}\quad\text{and}\quad B=\set{2,4,6}.
			\] Then $A\cap B=\emptyset$, that is, $A$ and $B$ are disjoint. Note that \begin{align*}
				\Pr[A]=\frac{3}{6}=\frac{1}{2},\quad&\Pr[B]=\frac{3}{6}=\frac{1}{2},\\
				\Pr[A\mid B]=\frac{\Pr[AB]}{\Pr[B]}=\frac{0}{1/2}=0,\quad&\Pr[B\mid A]=\frac{\Pr[BA]}{\Pr[A]}=\frac{0}{1/2}=0.
			\end{align*} Thus, $\Pr[A|B]\neq\Pr[A]$ and $\Pr[B|A]\neq\Pr[B]$. That is, $A$ and $B$ are not independent.
		\end{enumerate}
	\end{example}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Rule of Total Probailtiy}]
		\begin{proposition}
			Let events $A_1,\dots,A_n$ are satisfies
			\begin{enumerate}[(1)]
				\item $\Pr[A_i]>0$ for $i=1,\dots,n$
				\item $A_i\cap A_j=\emptyset$ for $i\neq j$
				\item $\bigcup_{i=1}^nA_i=\Omega$
			\end{enumerate} Then \begin{align*}
				\Pr[B]&=\sum_{i=1}^n\Pr[B|A_i]\Pr[A_i]\\
				&=\Pr[B|A_1]\Pr[A_1]+\Pr[B|A_2]\Pr[A_2]+\cdots+\Pr[B|A_n]\Pr[A_n].
			\end{align*}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		$B=B\cap\Omega=B\cap\del{\bigcup_{i=1}^nA_i}=\bigcup_{i=1}^n(B\cap A_i)$.
	\end{proof}
	\vspace{20pt}
	\subsection{Bayes' Theorem}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Bayes' Theorem}]
		\begin{theorem}
			\[
			P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\bar{B})P(\bar{B})}
			\] The posterior probability of $\bar{B}$ is then $P(\bar{B}|A)=1-P(B|A)$.
		\end{theorem}
	\end{tcolorbox}
	\begin{remark}
		\[
		\Pr[B\mid A]=\frac{\Pr[A|B]\cdot\Pr[B]}{\Pr[A]}\iff\text{Posterior}=\frac{\text{Likelihood}\cdot\text{Prior}}{\text{Evidence}}.
		\]
	\end{remark}
	
	\newpage
	\section{Random Variables}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Random Variable}]
		\begin{definition}
			A \textbf{random variable} $X$ is real-valued function on $\Omega$ the space of outcomes: \[
			X:\Omega\to\mathbb{R}.
			\] In other words, a random variable is a number whose value depends upon the outcome of a random experiment.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Sometimes, when convenient, we also allow $X$ to have the value $\infty$ or, more rarely, $-\infty$.
	\end{remark}

	\subsection{Discrete Random Variables}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Discrete Random Variable}]
	\begin{definition}
		A \textbf{discrete random variable} $X$ has finitely or countably many values $$
		x_i\quad\text{for}\quad i=1,2,\cdots
		$$ and $$p(x_i)=P(X=x_i)$$ with $i=1,2,\cdots,$ is called the \textbf{probability mass function of $X$}.
	\end{definition}
\end{tcolorbox}
	\begin{remark}
		A probability mass function $p$ has the following properties:\begin{enumerate}[(1)]
			\item $x=x_i, i\in I\implies p(x)=\Pr[X=x_i]$
			\item $0\leq p(x)\leq 1$, $\sum_{x\in X}p(x)=1$.
			\item $\Pr[a<X\leq b]=\sum_{a<x\leq b}p(x)$.
		\end{enumerate}
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Discrete Probability Distribution}]
		\begin{definition}
			The \textbf{probability distribution} of a discrete of a random variable $X$ is described as the function \[
			f(x_i) = P(X=x_i)
			\] which gives the probability for each value and satisfies: \begin{enumerate}
				\item $0\leq f(x_i)\leq 1$ for each value $x_i$ of $X$
				\item \(\dispsty\sum_{i=1}^kf(x_i)=1 \)
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}

	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Expectation(Mean) and Standard Deviation of a Probability Distribution}]
		\begin{definition}
			\ \begin{itemize}
				\item The \textbf{mean} of $X$ or \textbf{population mean} \begin{align*}
					E[X] &= \mu \\
					&= \sum(\text{Value}\ \times\ \text{Probability})=\sum x_if(x_i)
				\end{align*} Here the sum extends over all the distinct values $x_i$ of $X$.
				\item The \textbf{Variance and Standard Deviation of $\boldsymbol{X}$}
			 is given by \begin{align*}
			\sigma^2 &=\Var[X]=\sum(x_i-\mu)^2f(x_i) \\
			\sigma &=\sd[X]= +\sqrt{\Var[X]}
		\end{align*}
				\item \textbf{Alternative Formula for Hand calculation:} \[
				\sigma^2=\sum x_i^2f(x_i) - \mu^2
				\]
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{example}[\bf Calculating a Population Variance and Standard Deviation]
		Calculate the variance and the standard deviation of the distribution of $X$ that appears in the left two columns of below table.
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{c|c||XXXX||c}
				\toprule[1.2pt]
				$x$ & $f(x)$ & $xf(x)$ & $(x-\mu)$ & $(x-\mu)^2$ & $(x-\mu)^2f(x)$ & $x^2f(x)$\\
				\hline
				0&.1& 0 & -2&4&.4&0\\
				1&.2& .2& -1&1&.2&0.2\\
				2&.4& .8& 0&0&.0&1.6\\
				3&.2& .6& 1&1&.2&1.8\\
				4&.1& .4& 2&4&.4&1.6\\
				\hline
				Total&1.0&2.0 = $\mu$&&&$1.2=\sigma^2$&$5.2=\sum x^2f(x)$\\
			\end{tabularx}
		\end{center}\begin{align*}
			\Var(X)&=\sigma^2=1.2\quad& \sigma^2=5.2-(2.0)^2=1.2 \\
			\sd(X)&=\sigma=\sqrt{1.2}=1.095\quad& \sigma=\sqrt{1.2}=1.095
		\end{align*}
	\end{example}
	
	\newpage
	\subsection{Bernoulli}
	\begin{note}
		\ \begin{itemize}
			\item The sample space $S = \set{\ \text{S},\ \text{F}\ }$.
			\item The probability of success $p=P(S)$, the probability of failure $q=P(F)$.
			\item $0\leq p\leq 1$, $q=1-p$.
		\end{itemize}
	\end{note}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Binomial Distribution}]
		\begin{definition}
			The \textbf{binomial distribution} with $n$ trails and success probability $p$ is described by the function \[
			f(x) = P[X=x] = \binom{n}{x}p^x(1-p)^{n-x}
			\] for the possible values $x = 0, 1, \cdots, n$.
		\end{definition}
	\end{tcolorbox}
	\begin{example}[\bf An Example of the Binomial Distribution]
		The elementary outcomes of 4 samples, the associated probabilities, and the value of $X$ are listed as follows. \begin{center}
			\begin{tabular}{ccc ccc ccc ccc ccc}
				& FFFF &&& SFFF &&& SSFF &&& SSSF &&& SSSS & \\
				& 	   &&& FSFF &&& SFSF &&& SSFS &&& & \\
				& 	   &&& FFSF &&& SFFS &&& SFSS &&& & \\
				& 	   &&& FFFS &&& FSSF &&& FSSS &&& & \\
				& 	   &&&      &&& FSFS &&& &&& & \\
				& 	   &&&      &&& FFSS &&& &&& & \\
		\end{tabular}\end{center}
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{X||c|c|c|c|c}
				\toprule[1.2pt]
				Value of $X$ & 0 & 1 & 2 & 3 & 4 \\
				\hline\hline
				Probability of each outcome & $q^4$ & $pq^3$ & $p^2q^2$ & $p^3q$ & $p^4$ \\
				\hline
				Number of outcomes & $1=\binom{4}{0}$ & $4=\binom{4}{1}$ & $6=\binom{4}{2}$ & $4=\binom{4}{1}$ & $1=\binom{4}{4}$ \\
				\bottomrule[1.2pt] 
		\end{tabularx}\end{center}
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{X||c|c|c|c|c}
				\toprule[1.2pt]
				Value $x$ & 0 & 1 & 2 & 3 & 4 \\
				\hline\hline
				Probability $f(x)$ & $\binom{4}{0}p^0q^4$ & $\binom{4}{1}p^1q^3$ & $\binom{4}{2}p^2q^2$ & $\binom{4}{1}p^3q^1$ & $\binom{4}{4}p^4q^0$ \\
				\bottomrule[1.2pt]
			\end{tabularx}
		\end{center}
	\end{example}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Mean and Standard Deviation of the Binomial Distribution}]
		\begin{definition}
			\[
			X=X_1+X_2+\cdots+X_n\sim B(n,p)
			\] \begin{itemize}
				\item \(E[X]=E[X_1] + \cdots + E[X_n] = np \)
				\item \(\Var[X]=\Var[X_1] + \cdots + \Var[X_n] = npq \)
			\end{itemize} The binomial distribution with $n$ trials and success probability $p$ has \begin{align*}
			\text{Mean} &= np \\
			\text{Variance} &= npq = np(1-p) \\
			\text{sd} &= \sqrt{npq} \\
		\end{align*}
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Covariance and Correlation Coefficient of Two Random Variables}]
		\begin{definition}
			Let $X, Y$ be a random variables. Then \begin{enumerate}
				\item The covariance of them: \[\Cov(X,Y)=E[(X-\mu_1)(Y-\mu_2)] \]
				\item The correlation coefficient of them: \[\dispsty\Corr(X,Y)=E\left[\left(\frac{X-\mu_1}{\sigma_1}\right)\left(\frac{Y-\mu_2}{\sigma_2}\right)\right]=\frac{\Cov(X,Y)}{\sd(X)\sd(Y)} \]
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Note that $-1\leq\Corr(X,Y)\leq 1$ and \begin{align*}
			\Cov(X,Y) &= E[(X-\mu_1)(Y-\mu_2) ] \\
			&= E[XY-\mu_2X-\mu_1Y+\mu_1\mu_2] \\
			&= E[XY] - \mu_2E[X] -\mu_1E[Y] +\mu_1\mu_2 \\
			&= E[XY] - \mu_1\mu_2.
		\end{align*} That is, $\Cov(X,Y)=E[XY]-\mu_1\mu_2$.
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf }]
		\begin{proposition}
			 \ \begin{enumerate}[(1)]
				\item \(\Cov(aX+b,cY+d) = ac\cdot\Cov(X,Y) \)
				\item \(\Corr(aX+b,cY+d)=\begin{cases}
					\Corr(X,Y) &: ac>0 \\
					-\Corr(X,Y) &: ac<0 \\
				\end{cases} \)
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		\begin{enumerate}[(1)]
			\item \begin{align*}
				\Cov(aX+b,cY+d) &= E[(aX+b)-(a\mu_x+b)\cdot(cY+d-(c\mu_y+d))] \\
				&= E[a(X-\mu_x)\cdot c(Y-\mu_y)]
				= acE[(X-\mu_x)(Y-\mu_y)] \\
				&= ac\cdot\Cov(X,Y).
			\end{align*}
			\item Note that $\sigma_{aX+b}=\sqrt{\Var(aX+b)}=\sqrt{a^2\Var(X)}=\abs{a}\sigma_X$. Similarly $\sigma_{cY+d}=\abs{c}\sigma_Y$.\begin{align*}
				\Corr(aX+b,cY+d) = \frac{\Cov(aX+b,cY+d)}{\sigma_{aX+b}\sigma_{cY+d}} 
				= \frac{ac\cdot\Cov(X,Y)}{\abs{a}\sigma_X\abs{c}\sigma_Y}
				= \frac{ac}{\abs{ac}}\Corr(X,Y).
			\end{align*} Hence, $\Corr(aX+b,cY+d)=\begin{cases}
				\Corr(X,Y) &\text{if}\ ac>0 \\
				-\Corr(X,Y) &\text{if}\ ac<0
			\end{cases}$
		\end{enumerate}
	\end{proof}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Distribution of Sum of Two Probability Variables}]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item \(\Var(X+Y)=\Var(X)+\Var(Y)+2\Cov(X,Y) \)
				\item \(\Var(X-Y)=\Var(X)+\Var(Y)-2\Cov(X,Y) \)
			\end{enumerate}	
		\end{proposition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Two Probability Variables are Independent}]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
			\item \(E[XY]=E[X]\cdot E[Y] \)
			\item \(\Cov(X,Y)=0, \Corr(X,Y)=0 \)
			\item \(\Var(X\pm Y)=\Var(X)+\Var(Y) \)
		\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		\begin{enumerate}[(1)]
			\item \begin{align*}
				E[XY] &= \sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp(x_i,y_j) \\
				&= \sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp_1(x_i)p_2(y_j) \\
				&= \sum_{i=1}^\infty x_ip_1(x_i)\sum_{j=1}^\infty y_jp_2(y_j)  \\
				&=E[X]\cdot E[Y].
			\end{align*} 
			\item $\Cov(X,Y) = E[XY]-E[X]\cdot E[Y]=0$.
		\end{enumerate}
	\end{proof}
	
	\newpage
	\subsection{Continuous Random Variables}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Probability Density Function}]
		\begin{definition}
			The \textbf{probability density function} $f(x)$ describes the distribution of probability for a continuous random variable. It has the properties: \begin{enumerate}[(1)]
				\item The total area under the probability density curve is $1$.
				\item $P[a\leq X\leq b]$ = area under the probability density curve between $a$ and $b$.
				\item $f(x)\geq 0$ for all $x$.
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		With a continuous random variable, the probability that $X=x$ is \textbf{always} 0. It is only meaningful to speak about the probability that $X$ lies in an interval.
	\end{remark}
	\begin{remark}
		$p(x)$ is called \textbf{probability density function} of continuous random variable $X$ if $p(x)$ satisfies: \begin{enumerate}[(i)]
			\item $p(x)\geq 0$, $\dispsty\int_{-\infty}^{\infty}p(x)\ dx=1$,
			\item $P(a\leq X\leq b)=\dispsty\int_{a}^bp(x)\ dx$.
		\end{enumerate}
		Note that
		\begin{itemize}
			\item For any constant $c$, $\dispsty\int_{c}^cp(x)\ dx=0$.
			\item $P(a\leq X\leq b)=P(a< X\leq b)=P(a\leq X< b)=P(a< X< b)$.
		\end{itemize}
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Expectation of a Continuous Random Variable}]
		\begin{definition}
			\ \begin{itemize}
				\item Expectation(or Mean) of a Random Variable $X$\begin{itemize}
					\item a Discrete Random variable: $E[X]=\sum_{i=1}^\infty x_ip(x_i)$
					\item a Continuous Random variable: $E[X]=\int_{-\infty}^\infty xp(x)\ dx$
				\end{itemize}
				\item Expectation and Median of a Continuous Random Variable $X$\begin{itemize}
					\item Expectation($\mu=E[X]$): the balance point of the probability mass.
					\item Median: the value of $X$ that divides the area under the curve into halves.
				\end{itemize}
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	
	\subsection{Normal Random Variable}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Normal Random Variable}]
		\begin{definition}
			A random variable is \textbf{Normal with parameter $\mu\in\mathbb{R}$ and $\sigma^2>0$} or, in short, \textbf{$X$ is $N(\mu,\sigma^2)$}, if its density is the function given below. \[
			\text{Density}:\ f(x)=f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right],
			\] where $x\in(-\infty,\infty)$.
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf }]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item For$f(x)=\dispsty\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]$,\quad $
				\dispsty\int_{-\infty}^{\infty} f(x)\ dx=1.
				$
				\item $EX=\mu$.
				\item $\Var(X)=\sigma^2$.
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\subsection{The Normal Approximation to the Binomial}
	
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf The Normal Approximation to the Binomial}]
		\begin{theorem}
			When $np$ and $np(1-p)$ are both large, say, greater than 15, the binomial distribution is well approximated by the normal distribution having mean = $np$ and sd = $\sqrt{np(1-p)}$. That is, \[
			Z=\frac{X-np}{\sqrt{np(1-p)}}\ \text{is approximately}\ N(0,1).
			\]
		\end{theorem}
	\end{tcolorbox}
	\vspace{5pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Mean and Standard Deviation of $\overline{X}$}]
		\begin{definition}
			The distribution of the sample mean, based on a random sample of size $n$, has \begin{align*}
				E[\overline{X}] &=\mu&(=\text{Population mean}) \\
				\Var[\overline{X}] &=\frac{\sigma^2}{n}&\left(=\frac{\text{Population variance}}{\text{Sample size}}\right) \\
				\sd[\overline{X}] &=\frac{\sigma}{\sqrt{n}}&\left(=\frac{\text{Population standard deviation}}{\sqrt{\text{Sample size}}}\right) \\
			\end{align*} 
		\end{definition}
	\end{tcolorbox}
	
	\newpage
	\section{Central Limit Theorem}
	\subsection{CLT}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Central Limit Theorem}]
		\begin{theorem}
			Assume that $X,X_1,X_2,\dots$ are independent, identically distributed random variables, with finite $\mu=EX$ and $\sigma^2=\Var[X]$. Then, \[
			\lim\limits_{n\to\infty}\Pr\sbr{\frac{\sum_{i=1}^nX_i-\mu n}{\sigma\sqrt{n}}\leq x}=\Pr\sbr{Z\leq x},
			\] where $Z$ is standard Normal.
		\end{theorem}
	\end{tcolorbox}
	
	\subsection{Laws of Large Numbers}
	
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Weak Law of Large Numbers}]
		\begin{theorem}
			Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random variables, each having finite mean $E[X_i]=\mu$ and variance $\sigma^2$. Then, for any $\varepsilon>0$, \[
			\lim\limits_{n\to\infty}\Pr\sbr{\abs{\frac{\sum_{i=1}^n X_i}{n}-\mu}\geq\varepsilon}=0.
			\]
		\end{theorem}
	\end{tcolorbox}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Strong Law of Large Numbers}]
		\begin{theorem}
			Let $X_1,X_2,\dots$ be i.i.d. random variables with a finite first moment, $\mathbb{E}[X_i]=\mu$. Then \[
			\frac{1}{n}\sum_{i=1}^nX_i\to\mu\quad\text{almost surely as}\quad n\to\infty.
			\]
		\end{theorem}
	\end{tcolorbox}
	
	\newpage
	\section{Problem: RBG $\rightarrow$ RNG}
	\begin{exercise}[Uniform Distribution]
		Consider an algorithm 
		\begin{figure}[h!]
			\centering
			\begin{tabularx}{\textwidth}{cl}
				Step 1:& Drive the RBG independently $4$ times to generate a $4$-bit integer value $r$.\\
				Step 2:& \textbf{If} $r<10$ \textbf{then}\\
				&\tab\textbf{return} $r$\\
				&\textbf{else}\\
				&\tab \textbf{go to} Step 1
			\end{tabularx}
		\end{figure}\\
		Prove that \[
		\Pr[\mathsf{ouput}=n]=\frac{1}{10}
		\] for $n=0,1,2,\dots, 9$.
		\begin{proof}[\sol]
			Let \begin{itemize}
				\item $n\leq 2^k=m$ with $n=10, k=4$ and $m = 16$
				\item Output digit $=r\in[0,9]$.
			\end{itemize} Then \begin{table}[h!]\centering
				\begin{tabularx}{\textwidth}{c||c|c|c|c}
					\toprule[1.2pt]
					$\mathsf{output}$ & \textbf{1st iteration} & \textbf{2nd iteration} & $\cdots$ &\textbf{step iteration}\\
					\midrule
					$0$ & $\Pr[0]=\frac{1}{m}$ & $\Pr[0]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[0]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$1$ & $\Pr[1]=\frac{1}{m}$ & $\Pr[1]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[1]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$2$ & $\Pr[2]=\frac{1}{m}$ & $\Pr[2]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[2]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$\vdots$&$\vdots$&$\vdots$&$\cdots$&$\vdots$\\
					$n-1$ & $\Pr[n-1]=\frac{1}{m}$ & $\Pr[n-1]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[n-1]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					\bottomrule[1.2pt]
				\end{tabularx}
			\end{table}\\ Thus, \begin{align*}
				\Pr[\mathsf{output}=r]&=\frac{1}{m}+\frac{1}{m}\cdot\frac{m-n}{m}+\cdots+\frac{1}{m}\cdot\del{\frac{m-n}{m}}^s+\cdots\\
				&=\frac{1}{m}\sum_{s=0}^\infty\del{\frac{m-n}{m}}^s=\frac{1}{m}\sum_{s=0}^\infty\del{1-\frac{n}{m}}^s\\
				&=\frac{1}{m}\cdot\frac{1}{1-\del{1-\frac{n}{m}}}=\frac{1}{m}\cdot\frac{m}{n}\\
				&=\frac{1}{n}.
			\end{align*}
		\end{proof}
	\end{exercise}

	\newpage
	\chapter{Markov Chains}
	
	\section{Introduction}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Markov Chain}]
		\begin{definition}
			Let $$
			\langle X_n\rangle_{n\geq 0}:=\set{X_n:n=0,1,2,\cdots}
			$$ be a stochastic process over a countable set $S$. Let $\Pr[X]$ is the probability of the random variable $X$. Then $\langle X_n\rangle_{n\geq 0}$ satisfies \textbf{Markov property} if \[
			\Pr[X_{n+1}=x_{n+1}\mid X_0=x_0,\dots,X_n=x_n]=\Pr[X_{n+1}=x_{n+1}\mid X_n=x_n]
			\] for all $n\geq 0$ and all $x_0,\dots, x_{n+1}\in S$. Then $\markov{X_n}_{n\geq 0}$ is a \textbf{Markov chain}.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item The conditional probability of $X_{i+1}$ is dependent only upon $X_i$, and upon no earlier values of $\markov{X_n}$
			\item the state of $\markov{X_n}$ in the future is unaffected by its history.
			\item The set $S$ is called the \textbf{state space} of the Markov chain.
			\item The conditional probabilities $\Pr[X_{n+1}= y\mid X_n = x]$ are called the \textbf{transition probabilities}.
			\item Markov chain having \textbf{stationary transition probabilities}, \ie, $\Pr(X_{n+1}=y\mid X_n=x),$ is independent of $n$.
		\end{enumerate}
	\end{remark}
	\vspace{20pt}
	\newpage
	\begin{example}[\textit{The general two-state Markov chain}]
		There are two states $0$ and $1$ with transitions \begin{itemize}
			\item $0\to 1$ with probability $p$
			\item $0\to 0$ with probability $1-p$
			\item $1\to 0$ with probability $q$
			\item $1\to 1$ with probability $1-q$.
		\end{itemize} Thus we have \begin{align*}
		\Pr\sbr{X_{n+1}=1\mid X_n=0} &=p,\\
		\Pr\sbr{X_{n+1}=0\mid X_n=1} &=q,
	\end{align*} and $\Pr[X_0=0]=\pi_0(0)$. Since there are only two states, $0$ and $1$, it follows immediately that \begin{align*}
	\Pr\sbr{X_{n+1}=0\mid X_n=0}=1-p,\\
	\Pr\sbr{X_{n+1}=1\mid X_n=1}=1-q,
\end{align*} and $\pi_0(1)=\Pr[X_0=1]=1-\pi_0(0)$. The transition matrix has two parameters $p,q\in[0,1]$: \[
	\begin{bmatrix}
		T_{00} & T_{01}\\
		T_{10} & T_{11}\\
	\end{bmatrix}=\begin{bmatrix}
	1-p & p\\
	q & 1-q
\end{bmatrix}.
	\] Note that \begin{itemize}
		\item $\Pr[A]=\Pr[B\cap A]+\Pr[B^C\cap A]$
		\item $\Pr[A\cap B]=\Pr[A]\cdot\Pr[B\mid A]$
	\end{itemize} Then we observe that \begin{align*}
		\Pr\sbr{X_{n+1}=0}=&\Pr\sbr{X_n=0\land X_{n+1}=0}+\Pr\sbr{X_n=1\land X_{n+1}=0}\\
		=&\Pr[X_n=0]\Pr[X_{n+1}=0\mid X_n=0]\\
		&+\Pr[X_n=1]\Pr[X_{n+1}=0\mid X_n=1]\\
		=&\Pr[X_n=0]\cdot (1-p)+\Pr[X_n=1]\cdot q\\
		=&(1-p)\cdot \Pr[X_n=0]+q\cdot \del{1-\Pr[X_n=0]}\\
		=&(1-p-q)\cdot\Pr[X_n=0]+q.
	\end{align*} Now \begin{align*}
	\Pr[X_0=0]&=\pi_0(0),\\
	\Pr[X_1=0]&=(1-p-q)\pi_0(0)+q,\\
	\Pr[X_2=0]&=(1-p-q)\Pr[X_1=0]+q\\&=(1-p-q)^2\pi_0(0)+q\del{1+(1-p-q)},\\
	\Pr[X_3=0]&=(1-p-q)\Pr[X_2=0]+q\\&=(1-p-q)^3\pi_0(0)+q\del{1+(1-p-q)+(1-p-q)^2},\\
	&\vdots\\
	\Pr[X_n=0]&=(1-p-q)^n\pi_0(0) + q\sum_{j=0}^{n-1}(1-p-q)^j.
\end{align*} In the trivial case $p=q=0$, it is clear that for all $n$ \[
	\Pr[X_n=0]=\pi_0(0)\quad\text{and}\quad \Pr[X_n=1]=\pi_0(1).
\]	Suppose that $p+q>0$. By the formula $\displaystyle\sum_{j=0}^{n-1}r^j=\frac{1-r^n}{1-r}$ for the sum of a finite geometric progression, \[
	\sum_{j=0}^{n-1}(1-p-q)^j=\frac{1-(1-p-q)^n}{p+q}.
	\] Thus, \begin{align*}
		\Pr[X_n=0]&=\frac{q}{p+q}+(1-p-q)^n\cdot\del{\pi_0(0)-\frac{q}{p+q}},\\
		\Pr[X_n=1]&=\frac{p}{p+q}+(1-p-q)^n\cdot\del{\pi_0(1)-\frac{p}{p+q}}.
	\end{align*} Suppose that $p,q\notin\set{0,1}$. Then \[
	0<p+q<2\implies\abs{1-p-q}\leq 1.
	\] Then \[
	\lim\limits_{n\to\infty}\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\lim\limits_{n\to\infty}\Pr[X_n=1]=\frac{p}{p+q}.
	\] Suppose we want to choose $\pi_0(0)$ and $\pi_0(1)$ so that $\Pr[X_n=0]$ and $\Pr[X_n=1]$ are independent of $n$. To do this, we should choose $\pi_0(0)=q/(p+q)$ and $\pi_0(1)=p/(p+q)$.
	Thus if $\langle X_n\rangle_{n\geq 0}$ start with the initial distribution \[
	\pi_0=\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\pi_0(1)=\frac{p}{p+q},
	\] then for all $n$ \[
	\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\Pr[X_n=1]=\frac{p}{p+q}.
	\]
	\end{example}

	\newpage
	\begin{example}
		Let $n=2$ and $x_0,x_1,x_2\in\set{0,1}$. Then \begin{align*}
			&\Pr[X_0=x_0,X_1=x_1,X_2=x_2]\\=&
			\Pr[X_0=x_0,X_1=x_1]\cdot\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1]\\=&
			\Pr[X_0=x_0]\Pr[X_1=x_1\mid X_0=x_0]\cdot\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1].
		\end{align*} If the Markov property is satisfied, then \[
		\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1]=\Pr[X_2=2\mid X_1=x_1],
	\] which is determined by $p$ and $q$. In this case \[
	\Pr[X_0=x_0,X_1=x_1,X_2=x_2]=\Pr[X_0=x_0]\Pr[X_1=x_1\mid X_0=x_0]\Pr[X_2=x_2\mid X_1=x_1].
	\] Recall that the transition matrix with $p,q\in[0,1]$: \[
	\begin{bmatrix}
		T_{00} & T_{01}\\
		T_{10} & T_{11}\\
	\end{bmatrix}=\begin{bmatrix}
		1-p & p\\
		q & 1-q
	\end{bmatrix}.
	\] Then \begin{figure}[h!]\centering\renewcommand*{\arraystretch}{1.4}
		\begin{tabularx}{\textwidth}{XXX|c}
			\toprule[1.2pt]
			$x_0$ &$x_1$& $x_2$ & $\Pr[X_0=x_0,X_1=x_1,X_2=x_2]$\\
			\midrule
			0& 0& 0     &$\pi_0(0)(1-p)^2$\\
			\hline
			0& 0& 1     &$\pi_0(0)(1-p)p$\\
			\hline
			0& 1& 0     &$\pi_0(0)pq$\\
			\hline
			0& 1& 1     &$\pi_0(0)p(1-q)$\\
			\hline
			\hline
			1& 0& 0     &$(1-\pi_0(0))q(1-p)$\\
			\hline
			1& 0& 1     &$(1-\pi_0(0))qp$\\
			\hline
			1& 1& 0     &$(1-\pi_0(0))(1-q)q$\\
			\hline
			1& 1& 1     &$(1-\pi_0(0))(1-q)^2$\\
			\bottomrule[1.2pt]
		\end{tabularx}
	\end{figure}
	\end{example}
	\newpage
	\chapter{Examples of Markov chains}
	\newpage
	\chapter{Statistical Inferences}
	\newpage
	\chapter{Statistical Tests for Randomness}
	Statistical tests for random number generators (RNGs) are crucial in assessing the quality of the RNGs, which purport to produce random sequences. While it is not possible to mathematically prove that a sequence is random, these tests can reveal certain weaknesses. The tests work by evaluating sample outputs from the RNG against attributes expected from truly random sequences. The outcomes of these tests are probabilistic and not definite. If a sequence fails any of the tests, it may be rejected as non-random, or subjected to further testing. Passing all the tests allows the RNG to be accepted as random, or more precisely, not rejected, as it only provides probabilistic evidence of randomness.
	
	\section{Statistical tests for RNGs}
	Statistical tests for RNGs measure the quality of a bit generator's randomness. These tests are essential in identifying weaknesses in the generators by applying various statistical methods to the output sequences. For instance, a sequence that passes all the tests is not definitively random but is likely to exhibit characteristics of randomness, whereas a sequence that fails any test is potentially non-random and may require additional testing or rejection.
	
	\newpage
	\subsection{Golomb’s randomness postulates}
	Golomb’s randomness postulates are historical attempts to define necessary conditions for periodic pseudorandom sequences to appear random. They are not sufficient conditions for randomness but were among the first efforts to systematically address the randomness in sequences. These postulates serve as a fundamental basis for more complex tests and are critical in understanding the nature of pseudorandom sequences and their applications.
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf }]
		\begin{definition}
			Let \[
			s=s_0,s_1,s_2,\dots
			\] be an infnite sequence. The subsequence consisting of the first $n$ terms of $s$ is denoted by \[
			s^n=s_0,s_1,\cdots,s_{n-1}.
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		$s$ is the bit sequence if $s_i\in\set{0,1}$.
	\end{remark}
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf $N$-Periodic}]
		\begin{definition}
			The sequence $s=s_0,s_1,s_2,\dots$ is said to be \textbf{$N$-periodic} if \[
			s_i=s_{i+N}
			\] for all $i\geq 0$.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		If $s$ is a $N$-periodic sequence, then the \textbf{cycle} of $s$ is the subsequence $s^N$.
	\end{remark}
	\vspace{12pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Run - Gap / Block}]
		\begin{definition}
			Let $s$ be a sequence. \begin{itemize}
				\item A \textbf{run} of $s$ is a subsequence of $s$ consisting of consecutive $0$'s or $1$'s.
				\item A run of $0$'s is called a \textbf{gap}.
				\item A run of $1$'s is called a \textbf{block}.
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	\vspace{24pt}
	\newpage
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Autocorrelation Function}]
		\begin{definition}
			Let $s$ be a $N$-periodic sequence. The \textbf{autocorrelation function} of $s$ is the integer-value function $C(t):\set{s_i}\to\Z$ defined as \[
			C(t) = \frac{1}{N}\sum_{i=0}^{N-1}(2s_i - 1) \cdot (2s_{i+t} - 1)
			\] for $0\leq t\leq N-1$.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		
	\end{remark}
	\begin{remark}
		The autocorrelation function $C(t)$ measure the amount of similarity between the sequence $s$ and a shift of $s$ by $t$ positions. If $s$ is a random $N$-periodic sequence, then $\abs{N\cdot C(t)}$ can be expected to be quite small for all vlaue of $t\in\intoo{0,N}$. 
	\end{remark}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Golomb's randomness postulates}]
		\begin{definition}
			Let \( s \) be a $N$-periodic sequence. \textbf{Golomb's randomness postulates} are as follows:
			\begin{enumerate}[\textbf{R}1]
				\item In the cycle \( s^N \) of \( s \), the number of 1’s differs from the number of 0’s by at most 1.
				\item In the cycle \( s^N \), at least half the runs have length 1, at least one-fourth have length 2, at least one-eighth have length 3, and so on, as long as the number of runs so indicated exceeds 1. Moreover, for each of these lengths, there are (almost) equally many gaps and blocks.
				\item The autocorrelation function \( C(t) \) is two-valued. That is for some integer \( K \),
				\[
				N \cdot C(t) = \sum_{i=0}^{N-1}(2s_i - 1) \cdot (2s_{i+t} - 1)=
				\begin{cases} 
					N, & : t = 0, \\
					K, & : 1 \leq t \leq N - 1.
				\end{cases}
				\]
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}

	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf \textbf{Pseudo-Noise Sequence} (\textbf{pn-sequence})}]
		\begin{definition}
			A binary sequence which satisfies Golomb's randomness postulates is called a \textbf{pseudo-noise sequence} (\textbf{pn-sequence}).
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Pseudo-noise sequences arise in practice as output sequences of \textbf{maximum-length linear }
	\end{remark}
	
	The significance of Golomb’s randomness postulates lies in their ability to provide a framework for the evaluation of periodic sequences. A periodic sequence with period \( N \) is assessed based on the following criteria:
	
	\newpage
	\chapter{NIST SP 800-22}
	This document provides a detailed summary of the NIST Special Publication 800-22r1a. The publication is central to understanding the statistical test suite designed for evaluating random and pseudorandom number generators (RNGs) used in cryptographic applications.
	
	\section{Testing Strategy and Result Interpretation}
	The publication outlines a comprehensive strategy for statistically analyzing RNGs, which is crucial for ensuring the reliability and security of cryptographic systems.
	
	\subsection{Strategies for Statistical Analysis of an RNG}
	The strategy involves five key stages:
	\begin{enumerate}
		\item \textbf{Selection of a Generator:} Choosing a suitable hardware or software-based RNG.
		\item \textbf{Binary Sequence Generation:} Generating a set of binary sequences using the selected RNG.
		\item \textbf{Execution of the Statistical Test Suite:} Applying the NIST Statistical Test Suite to the generated sequences.
		\item \textbf{Examination of P-values:} Analyzing the P-values obtained from the test suite to evaluate the quality of the sequences.
		\item \textbf{Assessment: Pass/Fail Assignment:} Determining whether each sequence passes or fails the statistical tests based on P-value thresholds.
	\end{enumerate}
	
	\begin{remark}
		\ \begin{itemize}
			\item 
			$H0$ (null hypothesis) : The sequence being tested is random.
			\item $H_a$ (alternative hypothesis) : The sequence is not random.
			\item Level of significance : $\alpha = 0.01$ ($\alpha$ is chosen in the range $[0.001, 0.01]$.)
			\begin{itemize}
				\item $P$-value $< \alpha = 0.01\implies$ Reject $H_0$ (not random)
				\item $P$-value $\geq \alpha = 0.01\implies$ Accept $H_0$ (random)
			\end{itemize}
		\end{itemize}
		\begin{tikzpicture}
			% Draw horizontal line for alpha level
			\draw[red, thick] (0,2) -- (10,2) node[right] {$\alpha = 0.01$};
			
			% Annotate for Accept H0 and Reject H0
			\draw (5,1) node {Accept $H_0$ (random)};
			\draw (5,3) node {Reject $H_0$ (not random)};
			
			% Highlight areas for p-value comparison
			\fill[green, opacity=0.3] (0,0) rectangle (10,2);
			\fill[blue, opacity=0.3] (0,2) rectangle (10,4);
			
			% Add labels
			\draw (0,2) node[left] {$\alpha = 0.01$};
			
			% Add P-value lines for illustration (optional)
			\draw[dashed] (2,0) -- (2,4) node[above] {P-value $< \alpha$};
			\draw[dashed] (8,0) -- (8,4) node[above] {P-value $\geq \alpha$};
			
			% Draw axes
			\draw[thick,->] (0,0) -- (10,0) node[below right] {P-value};
			\draw[thick,->] (0,0) -- (0,4) node[left] {Probability};
		\end{tikzpicture}
	\end{remark}
	
	\section{The Interpretation of Empirical Results}
	\subsection{Proportion of Sequences Passing a Test}
%Proportion of Sequences Passing a Test
%Given the empirical results for a particular statistical test,
%compute the proportion of sequences that pass. For
%example, if 1000 binary sequences were tested (i.e., m =
%1000), α = 0.01 (the significance level), and 996 binary
%sequences had P-values ≥ .01, then the proportion is
%996/1000 = 0.9960.
%The range of acceptable proportions is determined using
%the confidence interval defined as, p̂ ± 3
%Figure 4-1: P-value Plot
%
%where p = 1-α, and m is the sample size. If the proportion
%falls outside of this interval, then there is evidence that the
%data is non-random. Note that other standard deviation
%values could be used. For the example above, the
%confidence interval is .99 ± 3 .99(.01 ) = .99 ± 0.0094392 (i.e., the proportion should lie above 0.9805607. This
%1000
%can be illustrated using a graph as shown in Figure 4-1. The confidence interval was calculated using a
%normal distribution as an approximation to the binomial distribution, which is reasonably accurate for
%large sample sizes (e.g., n ≥ 1000).

	\subsection{Uniform Distribution of P-values}
	
	\subsection{General Recommendations and Guidelines}
	Key recommendations include:
	\begin{itemize}
		\item Addressing programming errors in statistical tests.
		\item Dealing with underdeveloped statistical tests.
		\item Correcting flaws in RNG implementation.
		\item Ensuring accuracy in data processing for tests.
		\item Using quality mathematical routines for computing P-values.
		\item Making appropriate choices for test input parameters.
	\end{itemize}
	
	\subsection{Application of Multiple Tests}
	The publication highlights the importance of using multiple tests to ensure a wide-ranging assessment of RNGs. A study conducted by NIST revealed minimal redundancy among tests, confirming the suite's ability to thoroughly evaluate different aspects of randomness.
	
	\subsection{Conclusion}
	NIST SP 800-22r1a provides an essential framework for the evaluation of RNGs in cryptographic applications. Its structured approach, comprising detailed testing strategies and guidelines, is crucial for ensuring the integrity and security of cryptographic systems.
	
	\section{Useful Functions}
	
	\begin{itemize}
		\item \textbf{Standard Normal (Cumulative Distribution) Function}
		\[
		\Phi(z)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^ze^{-u^2/2}\ du.
		\]
		\item \textbf{Complementary Error Function}
		\[
		\textnormal{erfc}(z)=\frac{2}{\sqrt{\pi}}\int_{z}^{\infty}e^{-u^2}\ du.
		\]
	\end{itemize}

	\begin{enumerate}[(1)]
		\item Starting Point: The given definition of $\Phi(z)$.
		\[
		\Phi(z) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z} e^{-\frac{u^2}{2}}\ du.
		\]
		\item Change of Variable:  To relate 
		$\Phi(z)$ to 
		$\textnormal{erfc}
		(z)$, we make a change of variable in the integral. Let 
		$v=u/\sqrt{2}$, which implies 
		$u=\sqrt{2}v$ and 
		$du=\sqrt{2}dv$. \[
			\Phi(z) = \frac{1}{\sqrt{\pi}}\int_{-\infty}^{\frac{z}{\sqrt{2}}} e^{-v^2}\ dv.
			\]
		\item Expressing \(\Phi(z)\) in terms of \(\text{erfc}(z)\): he error function is defined as $\textnormal{erfc}(z)=\frac{2}{\sqrt{\pi}}\int_{z}^{\infty}e^{-u^2}\ du.$. Hence, the complementary error function is 
		$\textnormal{erfc}(z)=1-\textnormal{erf}(z)$.
		\[
			\text{erfc}(z) = 1 - \frac{2}{\sqrt{\pi}}\int_{0}^{z} e^{-u^2}\ du
		\]
		\item Connecting \(\Phi(z)\) and \(\text{erfc}(z)\): We observe that 
		$\Phi(z)$ integrates from 
		$-\infty$ to a positive value, while 
		$\textnormal{erfc}(z)$ integrates from a positive value to 
		$\infty$. They are complementary in nature. Therefore, we can write:
		\[
			\Phi\left(\frac{z}{\sqrt{2}}\right) = \frac{1}{2} \text{erfc}\left(-\frac{z}{\sqrt{2}}\right)
		\]
		\item Final Expression for \(\text{erfc}(z)\): Rearranging the last equation for 
		$\textnormal{erfc}(z)$, we get:
		\[
			\text{erfc}(z) = 2 \left( 1 - \Phi\left(\frac{z}{\sqrt{2}}\right) \right)
		\]
		\item Converting Back to Integral Form:
		Finally, substituting the integral form of 
		$\Phi(z)$ into the equation for 
		$\textnormal{erfc}(z)$, we arrive at the desired expression:
		\[
			\text{erfc}(z) = \frac{2}{\sqrt{\pi}}\int_{z}^{\infty}e^{-u^2}\ du
		\]
	\end{enumerate}
	
	\newpage
	\section{Frequency (Monobits) Test}
	
	\begin{itemize}
		\item The most basic test is that of the null hypothesis: in a sequence of independent identically distributed Bernoulli random variables the probability of ones is 1/2.
		\item By the classic \textbf{De Moivre-Laplace theorem} (\textit{Central Limit Theorem}), for a sufficiently large number of trials, the distribution of the binomial sum, normalized by $\sqrt{n}$, is closely approximated by a standard normal distribution.
		\item This test makes use of that approximation to assess the closeness of the fraction of 1's to 1/2.
		\item All subsequent tests are conditioned on having passed this first basic test.
		\item 
		\[
		X = 2\varepsilon-1, \quad S_n = X_1 + \cdots + X_n = 2(\varepsilon_1 + \cdots + \varepsilon_n) - n.
		\]
		
		\[
		\mathbb{E}[S_n] = 0, \quad \text{Var}(S_n) = n.
		\]
		
		\[
		\lim\limits_{n\to\infty}\Pr\sbr{\frac{S_n}{\sqrt{n}}\leq z}=\Phi(z)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{z}e^{-u^2/2}\ du,\quad \Pr\sbr{\frac{\abs{S_n}}{\sqrt{n}}\leq z}=2\Phi(z)-1.
		\]
		
		\item According to the test based on the statistic $s = |S_n|/\sqrt{n}$, evaluate the observed value $|s(\text{obs})| = |X_1 + \ldots + X_n|/\sqrt{n}$, and then calculate the corresponding \textbf{P-value}, which is
		
		\[
		2\left[1 - \Phi\left({|s(\text{obs})|}\right)\right] = \text{erfc}\left(\frac{|s(\text{obs})|}{\sqrt{2}}\right).
		\]
		
		Here, $\text{erfc}$ is the (complementary) error function \[
		\textnormal{erfc}(z)=\frac{2}{\sqrt{\pi}}\int_{z}^{\infty}e^{-u^2}\ du.
		\]
	\end{itemize}
	
	
	
	\section*{Frequency Test 수행 절차}
	
	\begin{enumerate}
		\item Conversion to \(\pm 1\): The zeros and ones of the input sequence (\(e\)) are converted to values of \(-1\) and \(+1\) and are added together to produce \(S_n = X_1 + X_2 + \ldots + X_n\), where \(X_i = 2e_i - 1\).
		
		For example, if \(e = 1011010101\), then \(n=10\) and \(S_n = (-1) + 1 + (-1) + 1 + (-1) + 1 + (-1) + 1 + (-1) + 1 = 2\).
		
		\item Compute the test statistic \(s_{\text{obs}} = \frac{|S_n|}{\sqrt{n}}\)
		
		For the example in this section, \(s_{\text{obs}} = \frac{|2|}{\sqrt{10}} \approx 0.632455532\).
		
		\item Compute \(P\)-value = \(\text{erfc}\left(\frac{s_{\text{obs}}}{\sqrt{2}}\right)\), where \(\text{erfc}\) is the complementary error function as defined in Section 5.5.3.
		
		For the example in this section, \(P\)-value = \(\text{erfc}\left(\frac{0.632455532}{\sqrt{2}}\right) \approx 0.527089\).
	\end{enumerate}

	\section*{Example of Frequency Test}
	
	\begin{itemize}
		\item[(input)] \( e = 110010100000111101101010100010000101010100011000010001010100110001001100010001100101000110010011100 \)
		\item[(input)] \( n = 100 \)
		\item[(processing)] \( S_{100} = -16 \)
		\item[(processing)] \( s_{\text{obs}} = 1.6 \)
		\item[(output)] \( P\text{-value} = 0.109599 \)
		\item[(conclusion)] \textbf{Since \( P\text{-value} \geq 0.01 \), accept the sequence as random.}
	\end{itemize}

	\section*{Frequency Test within a Block}
	
	The test seeks to detect localized deviations from the ideal 50\% frequency of 1's by decomposing the test sequence into a number of overlapping subsequences and applying a chi-square test for a homogeneous match of empirical frequencies to the ideal \( \frac{1}{2} \).
	
	\begin{itemize}
		\item Small \( P \)-values indicate large deviations from the equal proportion of ones and zeros in at least one of the substrings.
		\item The string of 0's and 1's (or equivalent -1's and 1's) is partitioned into a number of disjoint substrings.
		\item For each substring, the proportion of ones is computed.
		\item A chi-square statistic compares these substring proportions to the ideal \( \frac{1}{2} \).
		\item The statistic is referred to a chi-squared distribution with the degrees of freedom equal to the number of substrings.
	\end{itemize}
	
	The parameters of this test are \( M \) and \( N \), so that \( n = MN \), i.e., the original string is partitioned into \( N \) substrings, each of length \( M \).
	
	\begin{itemize}
		\item For each of these substrings, the probability of ones is estimated by the observed relative frequency of 1's, \( \pi_i \), for \( i = 1, \ldots, N \).
	\end{itemize}

	\section*{The reported \( P \)-value:}
	(where \texttt{igamc} is the incomplete gamma function)
	
	The reported \( P \)-value is computed as:
	\[
	\frac{\int_{\frac{\chi^2(\text{obs})}{2}}^{\infty} e^{-u} u^{(N/2)-1} du}{\Gamma(N/2)2^{N/2}} = \frac{\int_{\frac{\chi^2(\text{obs})}{2}}^{\infty} e^{-u} u^{N/2-1} du}{\Gamma(N/2)}
	= \texttt{igamc}\left( \frac{N}{2}, \frac{\chi^2(\text{obs})}{2} \right).
	\]
	
	\subsection*{Gamma Function}
	\[
	\Gamma(z) = \int_{0}^{\infty} t^{z-1}e^{-t}dt
	\]
	
	\subsection*{Incomplete Gamma Function}
	\[
	P(a,x) = \frac{\gamma(a,x)}{\Gamma(a)} = \frac{1}{\Gamma(a)}\int_{0}^{x}e^{-t}t^{a-1}dt
	\]
	where \(P(a,0) = 0\) and \(P(a,\infty) = 1\).
	
	\subsection*{Incomplete Gamma Function}
	\[
	Q(a,x) = 1 - P(a,x) = \frac{\Gamma(a,x)}{\Gamma(a)} = \frac{1}{\Gamma(a)}\int_{x}^{\infty}e^{-t}t^{a-1}dt
	\]
	where \(Q(a,0) = 1\) and \(Q(a,\infty) = 0\).

	
	\section*{Frequency Test within a Block 수행 절차}
	
	\begin{enumerate}
		\item Partition the input sequence into \(N = \left\lceil \frac{n}{M} \right\rceil\) non-overlapping blocks. Discard any unused bits.
		
		For example, if \(n = 10\), \(M = 3\) and \(e = 0110011010\), 3 blocks (\(N = 3\)) would be created, consisting of 011, 001 and 101. The final 0 would be discarded.
		
		\item Determine the proportion \(\pi_i\) of ones in each \(M\)-bit block using the equation
		\[
		\pi_i = \frac{\sum_{j=1}^{M} e_{(i-1)M+j}}{M}
		\]
		for \(1 \leq i \leq N\).
		
		For the example in this section, \(\pi_1 = \frac{2}{3}\), \(\pi_2 = \frac{1}{3}\), and \(\pi_3 = \frac{2}{3}\).
		
		\item Compute the \(\chi^2\) statistic: 
		\[
		\chi^2(\text{obs}) = 4M \sum_{i=1}^{N} \left(\pi_i - \frac{1}{2}\right)^2.
		\]
		
		For the example in this section, 
		\[
		\chi^2(\text{obs}) = 4 \times 3 \left(\left(\frac{2}{3} - \frac{1}{2}\right)^2 + \left(\frac{1}{3} - \frac{1}{2}\right)^2 + \left(\frac{2}{3} - \frac{1}{2}\right)^2\right) = 1.
		\]
	\end{enumerate}

	\subsection*{Compute \( P \)-value}
	Compute \( P \)-value \( = \texttt{igamc} \left( \frac{N}{2}, \frac{\chi^2(\text{obs})}{2} \right) \), where \texttt{igamc} is the incomplete gamma function for \( Q(a,x) \) as defined in Section 5.5.3.
	
	\textbf{Note:} When comparing this section against the technical description in Section 3.2, note that \( Q(a,x) = 1 - P(a,x) \).
	
	For the example in this section, \( P \)-value \( = \texttt{igamc} \left( \frac{3}{2}, \frac{7.2}{2} \right) = 0.801252 \).
	
	\section*{Example:}
	\begin{itemize}
		\item[(input)] \( e = 110010100000111101101010100010000101010100011000010001010100110001001100010001100101000110010011100 \)
		\item[(input)] \( n = 100 \)
		\item[(input)] \( M = 10 \)
		\item[(processing)] \( N = 10 \)
		\item[(processing)] \( \chi^2 = 7.2 \)
		\item[(output)] \( P \)-value \( = 0.706438 \)
		\item[(conclusion)] Since \( P \)-value \( > 0.01 \), accept the sequence as random.
	\end{itemize}

	\section*{Runs Test}
	
	This variant of a classic \textbf{nonparametric test} looks at ``runs'' defined as substrings of consecutive 1's and consecutive 0's, and considers whether the oscillation among such homogeneous substrings is too fast or too slow.
	
	The specific test used here is based on the distribution of \textbf{the total number of runs}, \( V_n \).
	\begin{itemize}
		\item For the fixed proportion \( \pi = \sum_{i=1}^{n} e_i / n \); \( \pi \) is an estimated parameter(Not assumed from \( H_0 \)),
		\[
		\lim_{n \to \infty} P\left(\frac{V_n - 2n\pi(1 - \pi)}{\sqrt{2n\pi(1 - \pi)}} \leq z\right) = \Phi(z).
		\]
	\end{itemize}
	
	To evaluate \( V_n \), define for \( k = 1, \ldots, n-1 \), \( r(k) = 0 \) if \( e_k = e_{k+1} \) and \( r(k) = 1 \) if \( e_k \neq e_{k+1} \).
	
	\[
	P\text{-value} = \text{erfc}\left(\frac{|V_n(\text{obs}) - 2n\pi(1 - \pi)|}{\sqrt{2n\pi(1 - \pi)}}\right).
	\]
	
	Large values of \( V_n(\text{obs}) \) indicate oscillation in the string of e's which is too fast; small values indicate oscillation which is too slow.
	
	\section*{Test Statistic and Reference Distribution}
	\( V_n\text{obs}: \) The total number of runs (i.e., the total number of zero runs + the total number of one-runs) across all \( n \) bits.
	The reference distribution for the test statistic is a \( \chi^2 \) distribution.
	
	\section*{Test Description}
	\textbf{Note:} The Runs test carries out a Frequency test as a prerequisite.
	\begin{enumerate}
		\item Compute the pre-test proportion \( \pi \) of ones in the input sequence: \( \pi = \frac{\sum e_j}{n} \).
		
		For example, if \( e = 1001101011 \), then \( n=10 \) and \( \pi = \frac{6}{10} = \frac{3}{5} \).
		
		\item Determine if the prerequisite Frequency test is passed: If it can be shown that \( | \frac{\pi - 1/2}{\sqrt{n/4}} | < t \), then the Runs test need not be performed (i.e., the test should not have been run because of a failure to pass test 1, the Frequency (Monobit) test). If the test is not applicable, then the \( P \)-value is set to 0.0000. Note that for this test, \( \frac{n}{4} \) has been pre-defined in the test code.
		
		For the example in this section, since \( t = \frac{\pi - 1/2}{\sqrt{n/4}} = 0.63246 \), then \( |t - 1/2| = |3/5 - 1/2| = 0.1 < t \).
		
		Since the observed value \( \pi \) is within the selected bounds, the runs test is applicable.
	\end{enumerate}

	\begin{enumerate}
		\setcounter{enumi}{2} % Starts the numbering from 3
		\item Compute the test statistic \( V_{n,\text{obs}} \):
		\[
		V_{n,\text{obs}} = \sum_{k=1}^{n-1} r(k+1), \text{ where } r(k)=0 \text{ if } e_k = e_{k+1}, \text{ and } r(k)=1 \text{ otherwise}.
		\]
		Since \( e = 001101011 \), then
		\[
		V_{10,\text{obs}} = (1+0+1+0+1+1+1+0+1) = 7.
		\]
		
		\item Compute \( P \)-value \( = \text{erfc} \left( \frac{|V_{n,\text{obs}} - 2n\pi(1 - \pi)|}{2\sqrt{2n\pi(1 - \pi)}} \right) \).
		
		For the example, \( P \)-value \( = \text{erfc} \left( \frac{2 \cdot 10 \cdot \frac{3}{5} \cdot \left(1 - \frac{3}{5}\right)}{2 \cdot \sqrt{2 \cdot 10 \cdot \frac{3}{5} \cdot \left(1 - \frac{3}{5}\right)}} \right) \approx 0.147232.
		\)
	\end{enumerate}
	
	\section*{Input Size Recommendation}
	It is recommended that each sequence to be tested consist of a minimum of 100 bits (i.e., \( n \geq 100 \)).
	
	\section*{Example}
	
	\begin{itemize}
		\item[(input)] \( e = 110010100000111101101010100010000101010100011000010001010100110001001100010001100101000110010011100 \)
		\item[(input)] \( n = 100 \)
		\item[(input)] \( \tau = 0.02 \)
		\item[(processing)] \( \pi = 0.42 \)
		\item[(processing)] \( V_{n,\text{obs}} = 52 \)
		\item[(output)] \( P\text{-value} = 0.500798 \)
		\item[(conclusion)] Since \( P\text{-value} \geq 0.01 \), accept the sequence as random.
	\end{itemize}
	
	\section{Binary Matrix Rank Test}
	
	\begin{itemize}
		\item The focus of the test is the rank of disjoint sub-matrices of the entire sequence.
		\item The purpose of this test is to check for \textcolor{red}{linear dependence} among fixed length substrings of the original sequence.
		\begin{itemize}
			\item Construct matrices of successive zeroes and ones from the sequence, and check for linear dependence among the rows or columns of the constructed matrices.
			\item The deviation of the rank - or rank deficiency - of the matrices from a theoretically expected value gives the statistic of interest.
		\end{itemize}
		\item The result states that the rank \( R \) of the \( M \times Q \) random binary matrix takes values \( r = 0, 1, 2, ..., m \), where \( m = \min(M, Q) \), with probabilities
		\[
		P_r = 2^{r(Q+M-r)-MQ}\prod_{i=0}^{r-1}\frac{(1-2^{i-Q})(1-2^{i-M})}{1-2^{i-r}},
		\]
		\begin{itemize}
			\item The probability values are fixed in the test suite code for \( M = Q = 32 \).
			\item The number \( M \) is then a parameter of this test, so that ideally \( n = M^2N \), where \( N \) is the new ``sample size''.
		\end{itemize}
		\item In practice, values for \( M \) and \( N \) are chosen so that the discarded part of the string, \( n - M^2N \), is fairly small.
	\end{itemize}
	
	\section*{Discrete Fourier Transform (Spectral) Test}
	
	\begin{itemize}
		\item The test described here is based on the \textbf{discrete Fourier transform}.
		\item It is a member of a class of procedures known as \textbf{spectral methods}.
		\item The Fourier test detects \textcolor{red}{[text redacted]} that would indicate a deviation from the assumption of randomness.
	\end{itemize}
	
	\subsection*{Test Purpose}
	\begin{itemize}
		\item The focus of this test is the peak heights in the Discrete Fourier Transform of the sequence.
		\item The purpose of this test is to detect periodic features (i.e., \textit{repetitive patterns} that are near each other) in the tested sequence that would indicate a deviation from the assumption of randomness.
		\item The intention is to detect whether the number of peaks exceeding the 95\% threshold is significantly different than 5\%.
	\end{itemize}
	\newpage
	\section{Testing Strategy and Result Interpretation}
	The publication details a five-stage strategy for the statistical analysis of RNGs, encompassing selection, generation, execution, examination, and assessment.
	
	\subsection{Strategies for Statistical Analysis of an RNG}
	\begin{itemize}
		\item Selection of a Generator
		\item Binary Sequence Generation
		\item Execution of the Statistical Test Suite
		\item Examination of P-values
		\item Assessment: Pass/Fail Assignment
	\end{itemize}
	
	\subsection{Interpretation of Empirical Results}
	Empirical results are interpreted through three scenarios: no deviation from randomness, clear deviation, or inconclusive results. The process involves examining sequence pass rates and P-value distributions.
	
	\subsection{General Recommendations and Guidelines}
	The document provides recommendations on addressing potential issues in statistical testing, such as programming errors, underdeveloped tests, RNG implementation flaws, data processing errors, poor mathematical routines, and incorrect parameter choices.
	
	\section{Application of Multiple Tests}
	A study on the interdependence of multiple tests revealed minimal redundancy, ensuring a comprehensive assessment of nonrandomness.
	
	\section{Conclusion}
	NIST SP 800-22r1a presents a comprehensive approach to evaluating RNGs for cryptographic applications, providing a structured testing strategy, methods for interpreting results, and guidelines to address common issues in statistical testing.
	
	
	
	
\newpage
\subsection{Golomb’s randomness postulates}
Solomon W. Golomb's randomness postulates are a cornerstone in the field of cryptography and sequence design, primarily due to their foundational role in the analysis and interpretation of sequences as random. These postulates provide a mathematical framework for evaluating the randomness of binary sequences. Specifically, they set forth criteria that a sequence must meet to be considered random. 

The postulates are as follows:

\begin{enumerate}
	\item The number of ones and zeros in the sequence should be approximately equal, which is necessary for the sequence to have no bias.
	\item The distance between consecutive ones should follow a geometric distribution. For binary sequences that are infinitely long, this implies that the probability of a 'gap' of length \( n \) between ones is \( (1/2)^{n+1} \), representing the lack of structure in the sequence.
	\item The sequence should be balanced, which means that for any binary substring, the number of occurrences of this substring and its complement should be approximately the same. This property is also referred to as the run property, where a 'run' is a substring of consecutive identical digits.
\end{enumerate}

Golomb’s postulates are integral in the design and analysis of pseudo-random number generators (PRNGs), as they serve as a benchmark for the sequence's randomness. A sequence that fulfills these postulates is considered to be a good candidate for cryptographic applications because it exhibits the unpredictability necessary for securing communications.

\textbf{Application in Cryptography:}
In cryptography, the randomness of key material is paramount. Golomb's postulates are used to ensure that the generated keys do not exhibit patterns or regularities that could be exploited by adversaries. By applying these postulates to evaluate the randomness of binary sequences, cryptographers can quantify the security level of cryptographic systems.

\textbf{Mathematical Implications:}
The postulates form the basis of several statistical tests, such as the runs test and the autocorrelation test. These tests are applied to binary sequences to check for the presence of patterns and correlations that would indicate non-randomness. The theoretical underpinnings of Golomb's postulates also contribute to the field of combinatorics and information theory, where they have implications for the construction of codes and error correction.

In conclusion, Golomb’s randomness postulates are not only historically significant but also remain highly relevant in the modern analysis of cryptographic systems. They

\chapter{Statistical Tests for Randomness}

\section{Introduction}
Statistical tests for randomness play a pivotal role in validating the quality and integrity of random number generators (RNGs). These tests serve as a benchmark against which RNGs are measured to ensure that their output sequences exhibit properties characteristic of true randomness. Despite the intrinsic limitations in proving randomness, these tests can effectively identify non-randomness in sequences, which is vital for applications in cryptography, simulations, and various stochastic modeling scenarios.

\section{Statistical tests for RNGs}
The purpose of statistical tests for RNGs is to analyze sequences for unpredictability, lack of patterns, and uniform distribution—traits expected from ideal random sequences. These tests range from simple frequency analysis to complex tests for serial correlation, and no single test can validate randomness conclusively. Hence, a battery of tests is typically employed, where failure of any test suggests non-randomness, prompting further scrutiny or rejection of the RNG.

\subsection{Golomb’s randomness postulates}
Golomb's randomness postulates, rooted in the theory of shift register sequences, lay foundational criteria for assessing the randomness of periodic sequences. These postulates dictate:

\begin{enumerate}
	\item \textbf{The Frequency Postulate}: The number of zeros and ones in a sequence should be approximately the same, reflecting the balance of a sequence.
	\item \textbf{The Run Postulate}: A sequence should contain runs of various lengths distributed according to expected probabilities. For instance, in a binary sequence, half of the runs should be of length one, one-fourth should be of length two, and so forth.
	\item \textbf{The Autocorrelation Postulate}: The autocorrelation function of the sequence should rapidly drop to zero as the shift increases, which implies that each bit should be independent of others at a certain distance.
\end{enumerate}

These postulates form the basis for more sophisticated tests, and while they are not sufficient to declare a sequence as random, they are necessary conditions. In particular, they address the uniformity and independence of a sequence, which are critical aspects in cryptographic applications.

Implementing these postulates in practical statistical tests has advanced the analysis of pseudorandom number generators (PRNGs), providing a methodology to assess their suitability for various applications. The evaluation of PRNGs against these postulates often involves chi-squared tests, spectral tests, and other statistical methodologies to detect non-random behavior in generated sequences.

	
	% End document
\end{document}